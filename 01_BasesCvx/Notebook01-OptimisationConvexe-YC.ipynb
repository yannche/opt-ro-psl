{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:rgb(46,139,87)\">Optimisation-RO</span>\n",
    "\n",
    "### <span style=\"color:rgb(46,139,87)\">Certification Chef de projet IA 2024-2025</span>\n",
    "\n",
    "\n",
    "# <span style=\"color:rgb(46,139,87)\">Notebook de la séance 1 - Optimisation convexe</span>\n",
    "\n",
    "\n",
    "Pour tout commentaire concernant ce notebook (y compris les typos), merci d'envoyer un mail à **yann.chevaleyre@lamsade.dauphine.fr**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <span style=\"color:rgb(46,139,87)\">Introduction</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cette session est d'illustrer des propriétés des problèmes d'optimisation convexe, via l'utilisation d'algorithmes existants pour résoudre des problèmes d'optimisation convexe apparemment complexes.\n",
    "\n",
    "*Note : Si les différentes parties de ce notebook sont construites de sorte à être indépendantes, les blocs sont fait pour être exécutés de manière séquentielle. On prendra notamment soin d'exécuter le bloc ci-dessous (qui importe des bibliothèques et fonctions utiles) en premier.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports des bibliothèques et fonctions utiles\n",
    "###############################################\n",
    "\n",
    "# Affichage\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt # Racine carrée\n",
    "from math import ceil # Partie entière supérieure\n",
    "from math import log # Logarithme\n",
    "\n",
    "# NumPy - Structures vectorielles et matricielles\n",
    "import numpy as np # Bibliothèque complète\n",
    "from numpy.random import multivariate_normal, randn, uniform, choice, normal, laplace # distributions de probabilité\n",
    "\n",
    "# SciPy - Calculs mathématiques efficaces\n",
    "from scipy.linalg import norm # normes classiques\n",
    "from scipy.linalg import toeplitz # matrices de Toeplitz\n",
    "from scipy.linalg import svdvals # décomposition en valeurs singulières\n",
    "from scipy.linalg import qr # factorisation QR\n",
    "from scipy.linalg import sqrtm # Racine carree de matrice\n",
    "from scipy.linalg import pinv # Inverse de matrice carree\n",
    "from scipy.optimize import check_grad # Vérification des dérivées\n",
    "from scipy import stats # distributions théoriques\n",
    "\n",
    "# Solveurs d'optimisation dans SciPy\n",
    "# - linprog : Programmation linéaire\n",
    "# - lsq_linear : Moindres carrés linéaires\n",
    "from scipy.optimize import fmin_l_bfgs_b, linprog, lsq_linear\n",
    "\n",
    "\n",
    "# Commande auxiliaire pour l'affichage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook utilise des routines des bibliothèques NumPy and Scipy *(sur Google Colab par défaut, à installer sur votre distribution locale).* Voici un lien vers un bon [tutoriel Numpy (en anglais)](https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf).\n",
    "\n",
    "**Fonctions NumPy utiles (cf documentation pour plus d'informations)**\n",
    "\n",
    "* *transpose* transposée de matrice (i.e. tableau NumPy bi-dimensionnel). On peut aussi utiliser X.T\n",
    "* *matmul* produit matrice-matrice (les dimensions doivent être compatibles).\n",
    "* *dot* produit matrice-vector (si les dimensions sont compatibles), aussi utilisable comme opérateur de produit scalaire entre deux vecteurs de même taille.\n",
    "* *np.ones((m,n))* matrice de taille m x n avec composantes égales à 1. \n",
    "* *np.zeros((m,n))* matrice de taille m x n avec composantes égales à 0.\n",
    "* *np.identity(n)* matrice identité de taille n x n.\n",
    "* *np.pi* $\\pi$.\n",
    "* *np.inf* nombre infini en mémoire.\n",
    "* *np.log* logarithme appliqué à chaque coordonnée des tableaux NumPy.\n",
    "* *np.exp* exponentielle appliquée à chaque coordonnée des tableaux NumPy.\n",
    "* *np.sum* somme des composantes d'un tableau NumPy (pour les matrices, somme selon une dimension)\n",
    "* *np.maximum(u,v)* renvoie un tableau NumPy array dont les composantes sont $max(u_i,v_i)$, si $u_i$ et $v_i$ sont celles de $u$ et $v$.\n",
    "* *np.concatenate* concatène des tableaux NumPy (vecteurs, matrices) de dimensions compatibles.\n",
    "* Si t est un tableau NumPy, la fonction *t.shape* renvoie les dimension(s) de ce tableau (utile lorsque l'on cherche à définir un tableau avec les mêmes dimensions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(46,139,87)\">Partie 1 - Maxima de vraisemblance et programmation convexe</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, on s'intéresse à une génération de données à partir d'une tendance linéaire. On considère ainsi un jeu de données $\\{(\\mathbf{a}_i,b_i)\\}_{i=1,\\dots,n}$, avec $\\mathbf{a}_i \\in \\mathbb{R}^d$ et $b_i \\in \\mathbb{R}$, et on suppose qu'il existe $\\mathbf{x}^* \\in \\mathbb{R}^d$ tel que \n",
    "$$\n",
    "    \\forall i=1,\\dots,n, \\quad b_i = \\mathbf{a}_i^T \\mathbf{x}^* + \\epsilon_i,\n",
    "$$\n",
    "où les $\\epsilon_i$ seront des variables aléatoires indépendantes et identiquement distribuées selon l'une des lois suivantes :\n",
    " - a) Loi gaussienne de moyenne $0$ et de variance $\\lambda>0$;\n",
    " - b) Loi de Laplace de moyenne $0$ et de paramètre d'échelle $\\lambda>0$;\n",
    " - c) Loi uniforme sur l'intervalle $[-\\lambda,\\lambda]$.\n",
    " \n",
    "Pour chacun de ces choix, on cherchera à calculer le problème de maximum de la vraisemblance via la résolution d'un problème d'optimisation convexe dédié, de la forme \n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "        {minimiser}_{\\mathbf{z} \\in \\mathbb{R}^d} &f(\\mathbf{z}) \\\\\n",
    "        {s.c.}  &\\mathbf{z} \\in \\mathcal{F}.\n",
    "    \\end{array}\n",
    "$$\n",
    "avec $d \\ge n$, $f$ une fonction convexe et $\\mathcal{F} \\subseteq \\mathbb{R}^d$ décrit par un ensemble de contraintes d'inégalité convexes et d'égalité linéaires (possiblement vide, auquel cas cet ensemble sera égal à $\\mathbb{R}^d$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(46,139,87)\">1.1 Modèles linéaires et données bruitées</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par définition, les $\\epsilon_i$ (et donc les $b_i$) sont des variables aléatoires i.i.d. La densité de probabilité du vecteur $\\mathbf{b}$ sera donc donnée par le produit des densités marginales des $b_i$. On a donc les résultats suivants pour chacun des cas possibles:\n",
    "\n",
    "a) $b_i$ suit une loi normale $\\mathcal{N}(\\mathbf{a}_i^T \\mathbf{x}^*, \\lambda)$.\n",
    "$$\n",
    "p(\\mathbf{b}) = \\frac{1}{(2\\pi\\lambda)^{n/2}} \n",
    "\\exp\\left[ - \\frac{\\|\\mathbf{b}-\\mathbf{A x^*}\\|^2}{2 \\lambda} \\right].\n",
    "$$\n",
    "b) $b_i$ suit une loi de Laplace $\\mathcal{Laplace}(\\mathbf{a}_i^T \\mathbf{x}^*, \\lambda)$.\n",
    "$$\n",
    "p(\\mathbf{b}) = \\frac{1}{(2\\lambda)^n} \n",
    "\\exp\\left[ - \\frac{\\|\\mathbf{b}-\\mathbf{A} \\mathbf{x}^*\\|_1}{\\lambda} \\right].\n",
    "$$\n",
    "c) $b_i$ suit une loi uniforme sur $[-\\lambda+\\mathbf{a}_i^T \\mathbf{x}^*,\\mathbf{a}_i^T \\mathbf{x}^*+\\lambda]$.\n",
    "$$\n",
    "p(\\mathbf{b}) = \\frac{1}{(2\\lambda)^n} \\prod_{i=1}^n \\mathbf{1}_{[-\\lambda+\\mathbf{a}_i^T \\mathbf{x}^*,\\mathbf{a}_i^T \\mathbf{x}^*+\\lambda]}(b_i).\n",
    "$$\n",
    "\n",
    "\n",
    "<!-- ![Distribution](images/distributions.png) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe de problèmes d'optimisation ci-dessous modélise les problèmes obtenus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe de modeles lineaires\n",
    "class GenerateModelLin(object):\n",
    "    \"\"\"\n",
    "    Construction d'une <classe de modeles lineaires avec differents types d'erreurs\n",
    "    \n",
    "    Attributs\n",
    "    ----------\n",
    "    A : matrice de donnees\n",
    "    n,d : dimensions de la matrice A\n",
    "    xstar : \"verite terrain\" (ground truth) du modele lineaire, vecteur de taille (d,1)\n",
    "    eps : vecteur des erreurs, de taille (n,1)\n",
    "    lbda : parametre de la loi de probabilite des erreurs (>0)\n",
    "    law : loi des entrees du vecteur eps\n",
    "        'gauss' (defaut): loi normale de moyenne 0 et de variance lambda\n",
    "        'laplace' : loi de Laplace (0,1/lambda)\n",
    "        'unif' : loi uniforme sur [-lambda,lambda]\n",
    "    b : observations bruitees, de taille (n,1)\n",
    "\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    # Instanciation de la classe\n",
    "    def __init__(self, A, xstar,lbda,law='gauss'):\n",
    "        self.A = A\n",
    "        self.n, self.d = A.shape\n",
    "        self.xstar = xstar\n",
    "        self.lbda=lbda\n",
    "        self.law=law\n",
    "\n",
    "        # Pour chacune des lois considerees, on calcule les valeurs des\n",
    "        # attributs b et eps\n",
    "        if law=='gauss':\n",
    "            self.eps = normal(0,lbda,size=n)\n",
    "        elif law=='laplace':\n",
    "            self.eps = laplace(0,lbda,size=n)\n",
    "        elif law=='unif':\n",
    "            self.eps = uniform(-lbda,lbda,size=n)\n",
    "        \n",
    "        self.b = self.A.dot(self.xstar)+self.eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valider cette implémentation au moyen du code ci-dessous, qui affiche le résultat lorsque $d=1$, et observer la distribution des points ainsi générés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On instancie plusieurs fois la classe pour afficher les resultats\n",
    "\n",
    "d=1\n",
    "n=40\n",
    "idx = np.arange(d)\n",
    "xstar = (-1)**idx * np.exp(-idx / 10.)\n",
    "corr = 0.5\n",
    "A = multivariate_normal(np.zeros(d), toeplitz(corr ** np.arange(0, d)), size=n)\n",
    "b = A.dot(xstar) #Tendance lineaire veritable\n",
    "lbda = 0.8\n",
    "\n",
    "mlg = GenerateModelLin(A,xstar,lbda,'gauss')\n",
    "mll = GenerateModelLin(A,xstar,lbda,'laplace')\n",
    "mlu = GenerateModelLin(A,xstar,lbda,'unif')\n",
    "\n",
    "if d==1:\n",
    "    # Trace des points initiaux\n",
    "    plt.figure()\n",
    "    plt.scatter(A[:,0],b,color='k',marker='o',label='Vrai modele')\n",
    "    plt.scatter(A[:,0],mlg.b,color='red',marker='^',label='Err. Gauss')\n",
    "    plt.scatter(A[:,0],mll.b,color='darkgoldenrod',marker='*',label='Err. Laplace')\n",
    "    plt.scatter(A[:,0],mlu.b,color='forestgreen',marker='s',label='Err. Unif')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #Trace des erreurs\n",
    "    plt.figure()\n",
    "    plt.plot(mlg.eps,'^',color='red',label='Err. Gauss')\n",
    "    plt.plot(mll.eps,'*',color='darkgoldenrod',label='Err. Laplace')\n",
    "    plt.plot(mlu.eps,'s',color='forestgreen',label='Err. Unif')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche les histogrammes des erreurs\n",
    "\n",
    "if d == 1:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "\n",
    "    # Histogramme et distribution Gaussienne\n",
    "    axs[0].hist(mlg.eps, bins=20, color='red', label='Err. Gauss', alpha=0.5, density=True)\n",
    "    axs[0].plot(x, stats.norm.pdf(x, 0, np.sqrt(lbda)), color='blue', label='Gauss. Théorique')\n",
    "    axs[0].set_title('Gaussian Errors')\n",
    "\n",
    "    # Histogramme et distribution de Laplace\n",
    "    axs[1].hist(mll.eps, bins=20, color='darkgoldenrod', label='Err. Laplace', alpha=0.5, density=True)\n",
    "    axs[1].plot(x, stats.laplace.pdf(x, 0, np.sqrt(lbda)), color='blue', label='Laplace Théorique')\n",
    "    axs[1].set_title('Laplace Errors')\n",
    "\n",
    "    # Histogramme et distribution Uniforme\n",
    "    axs[2].hist(mlu.eps, bins=20, color='forestgreen', label='Err. Unif', alpha=0.5, density=True)\n",
    "    axs[2].plot(x, stats.uniform.pdf(x, -lbda, 2 * lbda), color='blue', label='Unif. Théorique')\n",
    "    axs[2].set_title('Uniform Errors')\n",
    "\n",
    "    for i in range(2):\n",
    "        axs[i].set_xlim([-5, 5])\n",
    "        axs[i].legend()\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant chercher à calculer l'estimateur du maximum de vraisemblance pour les différentes lois possibles des erreurs. Pour ce faire, on reformule les problèmes de la façon la plus simple possible en des problèmes de minimisation convexe, puis on fait appel à des solveurs dédiés de la bibliothèque SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(46,139,87)\">1.2 - Reformulations convexes</span>\n",
    "\n",
    "Etant donné notre vecteur d'observations $\\mathbf{y} \\in \\mathbb{R}^n$, on considère une famille de lois de probabilité $p_{\\mathbf{x}}(\\cdot)$ paramétrée par un vecteur $\\mathbf{x} \\in \\mathbb{R}^d$. La vraisemblance est alors définie par\n",
    "$$\n",
    "    \\mathbf{x} \\; \\mapsto \\; p_{\\mathbf{x}}(\\mathbf{y}).\n",
    "$$\n",
    "et la *log-vraisemblance* correspond au logarithme de cette fonction. L'estimateur du maximum de vraisemblance est alors obtenu en résolvant le problème\n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \\ell(\\mathbf{x}):= \\ln\\left[p_{\\mathbf{x}}(\\mathbf{y})\\right].\n",
    "$$\n",
    "Dans tous les cas à venir, on obtient une formulation convexe en formant tout d'abord la log-vraisemblance, puis en prenant l'opposée pour obtenir un problème de minimisation. \n",
    "\n",
    "a) $\\epsilon_i \\sim \\mathcal{N}(0,\\lambda)$: ce cas correspond à un problème aux moindres carrés linéaires. À une constante près, le problème de maximisation de la log-vraisemblance correspond en effet à \n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} -\\frac{1}{2}\\left\\| \\mathbf{A x}-\\mathbf{b}\\right\\|^2,\n",
    "$$\n",
    "qui possède exactement le même ensemble de solutions que\n",
    "$$\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \\frac{1}{2}\\left\\|\\mathbf{A x}-\\mathbf{b}\\right\\|^2.\n",
    "$$\n",
    "Ce problème est un problème aux moindres carrés linéaires, qui est toujours convexe (sa matrice Hessienne étant $\\mathbf{A}^T \\mathbf{A} \\succeq \\mathbf{O}$).\n",
    "\n",
    "\n",
    "b) $\\epsilon_i \\sim {Laplace}(0,\\lambda)$: En utilisant la formule de la vraisemblance et sans considérer les termes constants, le problème de calcul du maximum de vraisemblance est équivalent à\n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} -\\frac{1}{\\lambda} \\left\\|\\mathbf{A} \\mathbf{x}-\\mathbf{b}\\right\\|_1,\n",
    "$$\n",
    "au sens où les problèmes possèdent le même ensemble de solutions.\n",
    "La quantité $\\lambda$ étant strictement positive, cet ensemble de solutions est également celui du problème\n",
    "$$\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \\left\\|\\mathbf{A} \\mathbf{x}-\\mathbf{b}\\right\\|_1 = \\sum_{i=1}^m \\left| \\mathbf{a}_i^T \\mathbf{x} - b_i \\right|,\n",
    "$$\n",
    "qui est un problème convexe mais dont l'objectif n'est pas partout dérivable. En rajoutant des variables supplémentaires, il est cependant possible de supprimer les valeurs absolues et d'obtenir ainsi un problème convexe avec contraintes linéaires :\n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "    {minimiser}_{\\substack{\\mathbf{x} \\in \\mathbb{R}^d \\\\ \\mathbf{t}^+ \\in \\mathbb{R}^n\n",
    "    \\\\ \\mathbf{t}^- \\in \\mathbb{R}^n}} \n",
    "    &\\sum_{i=1}^n \\left( [\\mathbf{t}^+]_i + [\\mathbf{t}^-]_i \\right) & \\\\\n",
    "    {s. c.} &[\\mathbf{t}^+]_i \\ge 0 &i=1,\\dots,n \\\\\n",
    "    &[\\mathbf{t}^-]_i \\ge 0 &i=1,\\dots,n \\\\\n",
    "    &[\\mathbf{t}^+]_i -[\\mathbf{t}^-]_i - (\\mathbf{a_i}^T \\mathbf{x} -b_i) = 0 &i=1,\\dots,n.\n",
    "    \\end{array}\n",
    "$$\n",
    "Ce problème est un programme linéaire, dont l'objectif et les $3n$ contraintes sont linéaires en les $d+2n$ variables $(\\mathbf{x},\\mathbf{t}^+,\\mathbf{t}^-)$.\n",
    "\n",
    "c) $\\epsilon_i \\sim U([-\\lambda,\\lambda])$: À une constante près, le problème de calcul de l'estimateur du maximum de vraisemblance s'écrit:\n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \n",
    "    \\sum_{i=1}^n \\ln\\left[\\mathbf{1}_{[-\\lambda+\\mathbf{a}_i^T \\mathbf{x},\\lambda+\\mathbf{a}_i^T \\mathbf{x}]}(b_i)\\right],\n",
    "$$\n",
    "qui est équivalent au problème de minimisation\n",
    "$$\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \n",
    "    -\\sum_{i=1}^n \\ln\\left[\\mathbf{1}_{[-\\lambda+\\mathbf{a}_i^T \\mathbf{x},\\lambda+\\mathbf{a}_i^T \\mathbf{x}]}(b_i)\\right],\n",
    "$$\n",
    "La fonction objectif prend la valeur $-\\inf$ en tout point $\\mathbf{w}$ pour lequel\n",
    "$y_i \\notin [-\\lambda+\\mathbf{a}_i^T \\mathbf{x},\\lambda+\\mathbf{a}_i^T \\mathbf{x}]$ pour \n",
    "au moins un indice $i \\in \\{1,\\dots,n\\}$, et la valeur 0 sinon. On peut donc écrire ce problème comme un problème de réalisabilité (c'est-à-dire à fonction objectif constante) de la manière suivante:\n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} &0 &\\\\\n",
    "    {s.c.} &-\\lambda + \\mathbf{a}_i^T \\mathbf{x} - b_i \\le 0 &i=1,\\dots,n\\\\\n",
    "    &-\\lambda - \\mathbf{a}_i^T \\mathbf{x} + b_i \\le 0 &i=1,\\dots,n.\n",
    "    \\end{array}\n",
    "$$\n",
    "Ce problème est convexe; il appartient à la catégorie des problèmes de programmation linéaires."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux classes ci-dessous permettent d'encoder les problèmes développés plus haut, et de les résoudre directement via des solveurs de la bibliothèque SciPy. La première de ces classes permet de représenter les problèmes aux moindres carrés linéaires de la forme\n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "        {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} &\\frac{1}{2}\\left\\|\\mathbf{A} \\mathbf{x}-\\mathbf{b}\\right\\|^2 \\\\\n",
    "        {s.c.} &\\mathbf{l}_b \\le \\mathbf{x} \\le \\mathbf{u}_b.\n",
    "    \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour les problemes aux moindres carres lineaires\n",
    "class LinLsq(object):\n",
    "    \"\"\"\n",
    "    Classe pour les problemes de moindres carres lineaires de la forme\n",
    "        minimiser (1/2)*||A x - b||^2\n",
    "        s.c. lb <= w <= ub\n",
    "    \n",
    "    Attributs\n",
    "    ----------\n",
    "    \n",
    "    A : Matrice du modele lineaire\n",
    "    b : Second membre du modele lineaire\n",
    "    bds : suite d'elements (l,u) formant des bornes sur chaque variable\n",
    "        Si bds =(c,d) avec c et d deux constantes, la meme borne est appliquee a toutes\n",
    "        les variables\n",
    "        Utiliser None pour des variables non bornees\n",
    "        \n",
    "    Methodes\n",
    "    --------\n",
    "    \n",
    "    resol : Resoudre le probleme via un appel a la methode lsq_linear de SciPy\n",
    "\n",
    "    \n",
    "    \"\"\"   \n",
    "\n",
    "    \n",
    "    \n",
    "    # Instanciation de la classe\n",
    "    def __init__(self, A, b,bds=(None,None)):\n",
    "        self.A = A\n",
    "        self.b = b\n",
    "        self.bds = bds\n",
    "        \n",
    "    # Resolution du probleme aux moindres carres lineaires sous-jacent via le solveur \n",
    "    # lsq_linear (note: ce solveur permet de preciser des bornes sur la variable)\n",
    "    def resol(self):\n",
    "        return lsq_linear(self.A, self.b, bounds=self.bds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe ci-dessous encode les programmes linéaires de la forme\n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "        \\min_{\\mathbf{x} \\in \\mathbb{R}^d} &\\mathbf{c}^T \\mathbf{x} \\\\\n",
    "        {s.c.} &\\mathbf{A}_i \\mathbf{x} - \\mathbf{b}_i \\le \\mathbf{0}\\\\\n",
    "        &\\mathbf{A}_e \\mathbf{x} - \\mathbf{b}_e = \\mathbf{0} \\\\\n",
    "        &\\mathbf{l}_b \\le \\mathbf{x} \\le \\mathbf{u}_b.\n",
    "    \\end{array}\n",
    "$$\n",
    "*Dans les logiciels de programmation linéaire, les contraintes de bornes sur les variables font généralement l'objet d'un traitement spécifique.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe pour les programmes lineaires\n",
    "class LinOpt(object):\n",
    "    \"\"\"\n",
    "    Classe pour les problemes de programmation lineaire de la forme\n",
    "        minimiser c^T x\n",
    "        s.c. A_e x - b_e = 0\n",
    "             A_i x - b_i <= 0\n",
    "             lb <= x <= ub\n",
    "    \n",
    "    Attributs\n",
    "    ----------\n",
    "    \n",
    "    c : vecteur definissant la fonction de cout\n",
    "    A_e : Matrice correspondant aux contraintes d'egalite lineaires\n",
    "    b_e : Second membre des contraintes d'egalite lineaires\n",
    "    A_i : Matrice correspondant aux contraintes d'inegalite lineaires\n",
    "    b_i : Second membre des contraintes d'inegalite lineaires\n",
    "    bds : suite d'elements (lb,ub) formant des bornes sur chaque variable\n",
    "        Si bds =(c,d) avec c et d deux constantes, la meme borne est appliquee a toutes\n",
    "        les variables\n",
    "        Utiliser None pour des variables non bornees\n",
    "    \n",
    "    Methodes\n",
    "    --------\n",
    "    \n",
    "    resol : Resoudre le probleme via un appel a la methode linprog de SciPy\n",
    "\n",
    "    \n",
    "    \"\"\"   \n",
    "\n",
    "    # Instanciation de la classe\n",
    "    def __init__(self, c, A_e=None, b_e=None, A_i=None, b_i=None, bds=(None,None)):\n",
    "        self.c=c\n",
    "        self.A_e=A_e\n",
    "        self.b_e=b_e\n",
    "        self.A_i=A_i\n",
    "        self.b_i=b_i\n",
    "        self.bds=bds\n",
    "        \n",
    "    # Resolution du probleme par une methode de points interieurs\n",
    "    def resol(self):\n",
    "        return linprog(c=self.c, A_ub=self.A_i, b_ub=self.b_i, A_eq=self.A_e, b_eq=self.b_e, bounds=self.bds, method='interior-point')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chacun des problèmes de minimisation convexe, on crée une instance appropriée et calculer la solution du problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation pour le probleme du maximum de vraisemblance avec erreurs gaussiennes.\n",
    "\n",
    "# On instancie la classe LinLsq pour representer le probleme considere\n",
    "\n",
    "lsqgauss = LinLsq(mlg.A,mlg.b) # Instance obtenue\n",
    "\n",
    "\n",
    "resgauss = lsqgauss.resol()\n",
    "\n",
    "# Ecart absolu avec la verite terrain\n",
    "print('Ecart avec verite terrain : ',norm(xstar-resgauss.x))\n",
    "\n",
    "# Valeur de la fonction de cout\n",
    "print('Fonction de cout : ', resgauss.cost)\n",
    "\n",
    "# Norme du gradient\n",
    "print('Norme du gradient : ', resgauss.optimality)\n",
    "\n",
    "\n",
    "if d==1:\n",
    "    # Affichage des points initiaux\n",
    "    plt.figure()\n",
    "    plt.scatter(A[:,0],b,color='k',marker='o',label='Vrai modele')\n",
    "    plt.scatter(A[:,0],mlg.b,color='red',marker='^',label='+Gauss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Trace des donnees et des predictions\n",
    "    plt.figure()\n",
    "    plt.plot(A[:,0],A.dot(xstar),color='k',label='Verite terrain')\n",
    "    plt.plot(A[:,0],A.dot(resgauss.x),color='red',label='MV Gauss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du maximum de vraisemblance avec erreurs de Laplace\n",
    "\n",
    "#  On instancie la classe LinOpt pour representer le probleme avec erreurs de Laplace\n",
    "#  tel qu'etabli en partie theorique\n",
    "\n",
    "dlap = mll.d+2*(mll.n)\n",
    "\n",
    "# Vecteur clap - clap[i]=0 pour x_i, clap[i]=1 pour t^+_i et  t^-_i\n",
    "clap= np.ones(dlap)\n",
    "clap[0:(mll.d)]=0\n",
    "\n",
    "# Matrice de contraintes d'egalite\n",
    "Im = np.identity(mll.n)\n",
    "Alap = np.concatenate((mll.A,-Im,Im),axis=1)\n",
    "blap = mll.b\n",
    "\n",
    "# Bornes \n",
    "laux = []\n",
    "for i in range(d):\n",
    "    laux.append((None,None))\n",
    "for i in range(d,dlap):\n",
    "    laux.append((0,None))\n",
    "bdslap = tuple(laux)\n",
    "\n",
    "# Instanciation\n",
    "lplaplace = LinOpt(c=clap,A_e=Alap,b_e=blap,bds=bdslap) # Instance obtenue\n",
    "\n",
    "\n",
    "reslaplace=lplaplace.resol()\n",
    "\n",
    "# Comparaison avec un estimateur Gaussien\n",
    "lsqlaplace = LinLsq(mll.A,mll.b) \n",
    "\n",
    "resLSQlaplace = lsqlaplace.resol()\n",
    "\n",
    "# Ecart absolu avec la verite terrain\n",
    "print('Ecart verite terrain (norme l1) : ',norm(xstar-reslaplace.x[0:mll.d]))\n",
    "print('Ecart verite terrain (norme l2) : ',norm(xstar-resLSQlaplace.x))\n",
    "\n",
    "# Valeur de la fonction de coit\n",
    "print('Fonction cout (norme l1) : ',reslaplace.fun)\n",
    "print('Fonction cout (norme l2) : ',resLSQlaplace.cost)\n",
    "\n",
    "\n",
    "if d==1:\n",
    "    # Affichage des points initiaux\n",
    "    plt.figure()\n",
    "    plt.scatter(A[:,0],b,color='k',marker='o',label='Vrai modele')\n",
    "    plt.scatter(A[:,0],mll.b,color='darkgoldenrod',marker='*',label='+Laplace')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage de la verite terrain et de son estimateur\n",
    "    plt.figure()\n",
    "    plt.plot(A[:,0],A.dot(xstar),color='k',label='Verite terrain')\n",
    "    plt.plot(A[:,0],A.dot(reslaplace.x[0:mll.d]),color='darkgoldenrod',label='Norme l1')\n",
    "    plt.plot(A[:,0],A.dot(resLSQlaplace.x),color='red',label='Norme l2')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du maximum de vraisemblance avec erreurs uniformes\n",
    "\n",
    "mlu = GenerateModelLin(A,xstar,lbda,'unif')\n",
    "\n",
    "#  On instancie la classe LinOpt pour representer le probleme avec erreurs uniformes\n",
    "#  tel qu'etabli en partie theorique\n",
    "\n",
    "\n",
    "# Vecteur cunif (objectif nul)\n",
    "cunif= np.zeros(mlu.d)\n",
    "\n",
    "# Matrice de contraintes d'inegalite\n",
    "Aunif = np.concatenate((mlu.A,-mlu.A),axis=0)\n",
    "bunif = mlu.lbda+ np.concatenate((mlu.b,-mlu.b),axis=0)\n",
    "\n",
    "# Instanciation\n",
    "lpunif = LinOpt(c=cunif,A_i=Aunif,b_i=bunif) # Instance obtenue\n",
    "\n",
    "\n",
    "resunif=lpunif.resol()\n",
    "\n",
    "# Ecart absolu avec la vérité terrain\n",
    "print(norm(xstar-resunif.x))\n",
    "\n",
    "# Valeur de la satisfaction des contraintes\n",
    "print(norm(resunif.slack))\n",
    "\n",
    "\n",
    "if d==1:\n",
    "    # Affichage des points initiaux\n",
    "    plt.figure()\n",
    "    plt.scatter(A[:,0],b,color='k',marker='o',label='Vrai modele')\n",
    "    plt.scatter(A[:,0],mlu.b,color='forestgreen',marker='s',label='+Unif')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage de la verite terrain et de son estimateur\n",
    "    plt.figure()\n",
    "    plt.plot(A[:,0],A.dot(xstar),color='k',label='Verite terrain')\n",
    "    plt.plot(A[:,0],A.dot(resunif.x),color='forestgreen',label='MV Unif')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Commentaire : La variante avec erreurs uniformes semble la plus simple à minimiser. Cependant, lorsque la valeur de $\\lambda$ est importante, le problème d'optimisation possède un grand nombre de solutions, parmi lesquelles il est impossible de faire un choix au sens de l'optimisation. À l'inverse, lorsque $\\lambda \\rightarrow 0$ et que la distribution tend vers une valeur ponctuelle, le problème se ramène à la résolution du système $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(46,139,87)\">1.3 - Régularisation et a priori</span>\n",
    "\n",
    "On considère maintenant au cas d'un modèle linéaire avec erreurs gaussiennes selon une loi normale $\\mathcal{N}(0,1)$, et on considère donc une famille de densités gaussiennes $\\mathcal{N}\\left(\\mathbf{A}\\mathbf{x},\\mathbf{I}\\right)$. \n",
    "\n",
    "On suppose de surcroît un a priori sur $\\mathbf{x}^*$; plus spécifiquement, on supposera \n",
    "que les entrées de $\\mathbf{x}$ sont i.i.d. selon l'une des lois suivantes :\n",
    "\n",
    "a) loi normale $\\mathcal{N}\\left(0,\\frac{1}{\\mu}\\right)$ avec $\\mu>0$;\n",
    "\n",
    "b) loi uniforme $U_{[\\mathbf{0},\\mathbf{1}]}$;\n",
    "\n",
    "c) loi $\\mathrm{Beta}(2,1)$.\n",
    "\n",
    "*On rappelle ici l'expression de la densité de probabilité d'une loi $\\mathrm{Beta}(2,1)$, dont le support est $[0,1]$ :*\n",
    "$$\n",
    "    \\forall z \\in [0,1], \\quad p(z) = 2 z.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Étant données une famille de distributions $\\{p_{\\mathbf{y}|\\mathbf{x}}(\\cdot)\\}$ sur $\\mathbb{R}^n$ paramétrée par $\\mathbf{x} \\in \\mathbb{R}^d$ et une famille de distributions a priori $\\{p_{\\mathbf{x}}(\\cdot)\\}$ sur $\\mathbb{R}^d$, le problème de calcul du maximum a posteriori est donné par\n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \\left\\{ \\ln\\left[p_{\\mathbf{y}|\\mathbf{x}}(\\mathbf{y})\\right] + \\ln\\left[ p_{\\mathbf{x}}(\\mathbf{x})\\right] \\right\\}.\n",
    "$$\n",
    "\n",
    "On utilise la formulation classique du problème de maximum a posteriori en omettant les termes constants, qui ne jouent aucun rôle dans l'optimisation.  On obtient ainsi les trois résultats suivants :\n",
    "\n",
    "a) Le premier cas donne\n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} -\\frac{1}{2}\\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\|^2 - \\frac{\\mu}{2}\\|\\mathbf{x}\\|^2,\n",
    "$$\n",
    "qui est équivalent au problème fortement convexe\n",
    "$$\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \\frac{1}{2}\\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\|^2 + \\frac{\\mu}{2}\\|\\mathbf{x}\\|^2.\n",
    "$$\n",
    "On notera (utile pour la partie pratique) que ce problème se met sous la forme d'un problème aux moindres carrés comme suit :\n",
    "$$\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \\frac{1}{2}\\left\\| \\mathbf{H}\\mathbf{x}-\\mathbf{g}\\right\\|^2, \\quad \\mathbf{H} := \\left( \\mathbf{A}^T \\mathbf{A} + \\mu \\mathbf{I} \\right)^{1/2}, \n",
    "    \\quad \\mathbf{g} := \\mathbf{H}^{-1} \\mathbf{A}^T \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "b) Les densités a priori sont de la forme\n",
    "$$\n",
    "    p_{\\mathbf{x}}(\\mathbf{x}) =  \\prod_{i=1}^d \\mathbf{1}_{[0,1]}(x_i).\n",
    "$$\n",
    "Le problème d'estimation du maximum a posteriori s'écrit alors\n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} -\\frac{1}{2}\\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\|^2 + \\sum_{i=1}^d \\ln\\left[\\mathbf{1}_{[0,1]}(x_i)\\right],\n",
    "$$\n",
    "on peut reformuler le problème comme un problème de minimisation avec contraintes de bornes:\n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} \n",
    "    &\\frac{1}{2}\\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\|^2 & \\\\\n",
    "    {s.c.} &0 \\le x_i \\le 1 &i=1,\\dots,d.\n",
    "    \\end{array}\n",
    "$$\n",
    "\n",
    "c) En utilisant la formule de la densité bêta et le fait que l'a priori sur $\\mathbf{x}$ suppose que les entrées sont i.i.d., on obtient\n",
    "$$\n",
    "    p_{\\mathbf{x}}(\\mathbf{x}) = 2^d \\prod_{i=1}^d x_i \\mathbf{1}_{[0,1]}(x_i).\n",
    "$$\n",
    "En introduisant cela dans la formule du problème d'estimation du maximum a posteriori et en enlevant les termes constants de l'objectif, on obtient le problème\n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} &-\\frac{1}{2}\\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\|^2 + \\sum_{i=1}^d \\ln(x_i) & \\\\\n",
    "    {s.c.} &0 \\le x_i \\le 1 &i=1,\\dots,n.\n",
    "    \\end{array}\n",
    "$$\n",
    "qui est équivalent au problème convexe\n",
    "$$\n",
    "    \\begin{array}{lll}\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} &\\frac{1}{2}\\left\\|\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right\\|^2 - \\sum_{i=1}^d \\ln(x_i)& \\\\\n",
    "    {s.c.} &0 \\le x_i \\le 1 &i=1,\\dots,n.\n",
    "    \\end{array}\n",
    "$$\n",
    "Ce problème est convexe,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous, basé sur la classe LinLsq, calcule l'estimateur du maximum a posteriori pour les choix a) (avec $\\mu=10$) et b), et compare les valeurs obtenues à celle sans a priori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciation pour le problème du maximum de vraisemblance \n",
    "# avec erreurs gaussiennes standards.\n",
    "\n",
    "mlgs = GenerateModelLin(A,xstar,1,'gauss')\n",
    "lsqmvg = LinLsq(mlgs.A,mlgs.b) # Probleme sans a priori\n",
    "\n",
    "\n",
    "# On instancie la classe LinLsq pour encoder les problemes de calcul du maximum \n",
    "# a posteriori pour chacun des deux a priori\n",
    "\n",
    "mu = 10\n",
    "H = np.matmul(mlgs.A.T,mlgs.A)+mu*np.identity(d)\n",
    "H = sqrtm(H)\n",
    "Hi = pinv(H)@(mlgs.A.T)\n",
    "lsqmapgg = LinLsq(H,Hi.dot(mlgs.b))#Instance avec a priori gaussien (mu=10)\n",
    "\n",
    "lsqmapgu = LinLsq(mlgs.A,mlgs.b,bds=(0,1))\n",
    "\n",
    "\n",
    "\n",
    "resmvg = lsqmvg.resol()\n",
    "resmapgg = lsqmapgg.resol()\n",
    "resmapgu = lsqmapgu.resol()\n",
    "\n",
    "# Ecart absolu avec la vérité terrain\n",
    "print('Ecart MV : ', norm(xstar-resmvg.x))\n",
    "print('Ecart MAP Gauss : ', norm(xstar-resmapgg.x))\n",
    "print('Ecart MAP Unif : ', norm(xstar-resmapgu.x))\n",
    "\n",
    "# Valeur de la fonction de coût\n",
    "print('Fonction de cout MV : ',resmvg.cost)\n",
    "print('Fonction de cout MAP Gauss : ',resmapgg.cost)\n",
    "print('Fonction de cout MAP Unif : ',resmapgu.cost)\n",
    "\n",
    "# Norme du gradient\n",
    "print('Norme gradient MV :', resmvg.optimality)\n",
    "print('Norme gradient MAP Gauss :', resmapgg.optimality)\n",
    "print('Norme gradient MAP Unif :', resmapgu.optimality)\n",
    "\n",
    "\n",
    "if d==1:\n",
    "    # Affichage des points initiaux\n",
    "    plt.figure()\n",
    "    plt.scatter(A[:,0],b,color='k',marker='o',label='Vrai modele')\n",
    "    plt.scatter(A[:,0],mlgs.b,color='red',marker='^',label='MV Gauss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage de la verite terrain et des predictions\n",
    "    plt.figure()\n",
    "    plt.plot(A[:,0],A.dot(xstar),color='k',label='Verite terrain')\n",
    "    plt.plot(A[:,0],A.dot(resmvg.x),color='red',label='MV Gauss')\n",
    "    plt.plot(A[:,0],A.dot(resmapgg.x),color='crimson',label='MAP Gauss')\n",
    "    plt.plot(A[:,0],A.dot(resmapgu.x),color='chocolate',label='MAP Unif')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:rgb(46,139,87)\">Partie 2 - Régression non linéaire et logistique</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la partie précédente, nous avons vu un choix d'a priori qui conduisait à un problème sans contraintes avec un objectif non linéaire (et dont le domaine n'était pas nécessairement $\\mathbb{R}^d$).\n",
    "\n",
    "En *régression logistique*, on cherche à réaliser une classification binaire, c'est-à-dire que l'on dispose d'un jeu de données $\\{(\\mathbf{a}_i,b_i)\\}_{i=1}^n$ où $\\mathbb{a}_i \\in \\mathbb{R}^d$ et $b_i \\in \\{0,1\\}$.\n",
    "\n",
    "Il s'agit alors de postuler que les observations $b_i$ sont indépendantes, et que $b_i$ suit une loi de Bernoulli définie par\n",
    "$$\n",
    "    \\mathbb{P}(b_i=0) = \n",
    "    \\frac{\\exp[-\\mathbf{a}_i^T \\mathbf{x}^*]}{1+\\exp[-\\mathbf{a}_i^T \\mathbf{x}^*]}.\n",
    "$$\n",
    "On parlera de $\\mathbf{x}^*$ comme du paramètre de la loi, et il s'agit de celui que l'on souhaitera estimer. On consid\\`erera la famille de densit\\'es de probabilit\\'e de la forme ci-dessus, paramétrée par un vecteur $\\mathbf{x} \\in \\mathbb{R}^d$. On supposera aussi un a priori gaussien sur les entrées de $\\mathbf{x}$ suivant une loi $\\mathcal{N}\\left(\\mathbf{0},\\tfrac{1}{\\mu}\\mathbf{I}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de vraisemblance est définie pour tout $\\mathbf{w} \\in \\mathbb{R}^d$ par\n",
    "$$\n",
    "    p_{\\mathbf{x}}(\\mathbf{b}):= \\prod_{i=1}^n \\left[\\frac{\\exp[-\\mathbf{a}_i^T \\mathbf{b}^*]}{1+\\exp[-\\mathbf{a}_i^T \\mathbf{x}^*]}\\right]^{1-b_i} \n",
    "    \\left[\\frac{1}{1+\\exp[-\\mathbf{a}_i^T \\mathbf{x}^*]}\\right]^{b_i} \n",
    "    = \\prod_{i: b_i=0} \\frac{\\exp[-\\mathbf{a}_i^T \\mathbf{x}^*]}{1+\\exp[-\\mathbf{a}_i^T \\mathbf{x}^*]}\\, \\prod_{i: b_i=1} \\frac{1}{1+\\exp[-\\mathbf{a}_i^T \\mathbf{x}^*]}\n",
    "$$\n",
    "En passant au logarithme, on obtient le problème de maximisation suivant\n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbf{R}^d} \\left\\{ \\sum_{i: b_i=0} \\ln\\left( \\frac{\\exp(-\\mathbf{a}_i^T \\mathbf{x})}{1+\\exp(-\\mathbf{a}_i^T \\mathbf{x})}\\right) + \\sum_{i: b_i=1} \\ln\\left( \\frac{1}{1+\\exp(-\\mathbf{a}_i^T \\mathbf{x})} \\right) \\right\\} = \n",
    "    -\\sum_{i: b_i=0}  \\mathbf{a}_i^T \\mathbf{x} -\\sum_{i=1}^n \\ln(1+\\exp(-\\mathbf{a}_i^T \\mathbf{x})).\n",
    "$$\n",
    "La fonction $\\mathbf{x} \\mapsto -\\mathbf{a}_i^T \\mathbf{x}$ est linéaire donc concave. Par ailleurs, la fonction $t \\mapsto \\ln(1+\\exp(-t))$ est convexe, donc par composition $\\mathbf{x} \\mapsto -\\ln(1+\\exp(-\\mathbf{a}_i^T \\mathbf{x}))$ est concave. Ce problème de maximisation de la log-vraisemblance est donc bien concave.\n",
    "\n",
    "- La famille des densités a priori correspond à une loi normale : on aura donc\n",
    "$$\n",
    "    p_{\\mathbf{x}}(\\mathbf{x}) = \\left[ \\frac{1}{\\sqrt{2\\pi\\mu}} \\right]^d\n",
    "    \\exp\\left[- \\frac{\\mu}{2} \\|\\mathbf{x}\\|^2 \\right].\n",
    "$$\n",
    "Par conséquent, le problème d'estimation du maximum de vraisemblance s'écrira \n",
    "$$\n",
    "    {maximiser}_{\\mathbf{x} \\in \\mathbb{R}^d} -\\sum_{i: b_i=0}  \\mathbf{a}_i^T \\mathbf{x} -\\sum_{i=1}^n \\ln(1+\\exp(-\\mathbf{a}_i^T \\mathbf{x})) - \\frac{d}{2}\\ln(2\\pi\\mu) - \\frac{\\mu}{2} \\|\\mathbf{x}\\|^2\n",
    "$$\n",
    "qui est équivalent au problème convexe\n",
    "$$\n",
    "    {minimiser}_{\\mathbf{x} \\in \\mathbb{R}^d} f(\\mathbf{x}):= \\sum_{i: b_i=0}  \\mathbf{a}_i^T \\mathbf{x} +\\sum_{i=1}^n \\ln(1+\\exp(-\\mathbf{a}_i^T \\mathbf{x})) + \\frac{\\mu}{2}\\|\\mathbf{x}\\|^2.\n",
    "$$\n",
    "\n",
    "\n",
    "- La dérivée de la fonction $t \\mapsto \\ln(1+\\exp(-t))$ est $t \\mapsto \\frac{-\\exp(-t)}{1+\\exp(-t)} = -\\frac{1}{1+\\exp(t)}$. Par composition avec la fonction linéaire $\\mathbf{x} \\mapsto \\mathbf{a}_i^T \\mathbf{x}$, on voit que le gradient de la fonction \n",
    "$\\mathbf{x} \\mapsto \\ln\\left(1+ \\exp(-\\mathbf{a}_i^T \\mathbf{x})\\right)$ est égal à\n",
    "$$\n",
    "- \\frac{1}{1 + \\exp(\\mathbf{a}_i^T \\mathbf{x})} \\mathbf{a}_i.\n",
    "$$\n",
    "Par conséquent, on obtient\n",
    "$$\n",
    "\\nabla f(\\mathbf{x}) = \\sum_{i:b_i=0} \\mathbf{a}_i - \\sum_{i=1}^n  \\frac{1}{1 + \\exp(\\mathbf{a}_i^T \\mathbf{x})} \\mathbf{a}_i + \\mu \\mathbf{x}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Python servant a representer les problemes de régression non linéaires \n",
    "# basés sur des modèles linéaires\n",
    "class RegNonLin(object):\n",
    "    \"\"\"\n",
    "    Classe pour les problemes de regression avec objectif non lineaire, de la forme\n",
    "        minimiser f(x;A,b)+ mu*r(x)\n",
    "        \n",
    "    ou f represente un terme d'attache aux donnees et r une regularisation.\n",
    "    \n",
    "    Attributs\n",
    "    ----------\n",
    "    \n",
    "    A : Matrice de donnees (\"attributs/caracteristiques\")\n",
    "    b : Vecteur de donnees (\"labels\")\n",
    "    mu : parametre de regularisation\n",
    "    loss : Forme du terme d'attache aux donnees\n",
    "        'lin' : moindres carres (comme en regression lineaire) (1/2)||Ax-y||^2\n",
    "        'log' : perte logistique\n",
    "    reg : Forme du terme de regularisation\n",
    "        'none': pas de regularisation\n",
    "        'gauss' : regularisation en (1/2)||x||^2\n",
    "\n",
    "    Methodes\n",
    "    ---------\n",
    "    \n",
    "    f_reg : Evaluation du terme de regularisation\n",
    "    g_reg : Gradient du terme de regularisation\n",
    "    fun : Valeur de la fonction objectif du probleme\n",
    "    grad : Gradient de la fonction objectif du probleme\n",
    "    \n",
    "    \"\"\"   \n",
    "\n",
    "    \n",
    "   \n",
    "    # Instanciation de la classe\n",
    "    def __init__(self, A, b,mu=1,loss='log',reg='none'):\n",
    "        self.A = A\n",
    "        self.n, self.d = A.shape\n",
    "        self.b = b\n",
    "        self.mu = mu\n",
    "        self.loss = loss\n",
    "        self.reg = reg\n",
    "    \n",
    "    # Fonction de regularisation/d'a priori\n",
    "    def f_reg(self, x):\n",
    "\n",
    "        # Calcul de la valeur du terme de regularisation en x (selon self.reg)\n",
    "        if self.reg=='none':\n",
    "            return 0\n",
    "        elif self.reg=='gauss':\n",
    "            return  self.mu * norm(x) ** 2 / 2.\n",
    "\n",
    "        \n",
    "    # Gradient du terme a priori\n",
    "    def g_reg(self,x):\n",
    " \n",
    "        # Calcul du gradient du terme de regularisation en x (selon self.reg)\n",
    "        if self.reg=='none':\n",
    "            return np.zeros(self.d)\n",
    "        elif self.reg=='gauss':\n",
    "            return (self.mu)*x\n",
    "   \n",
    "    \n",
    "    # Valeur de l'objectif du probleme (maximum de vraisemblance ou a posteriori)\n",
    "    def fun(self,x):\n",
    "\n",
    "        # Calcul de la fonction objectif en x (perte+regularisation)\n",
    "        if self.loss=='lin':\n",
    "            return norm(self.A.dot(x)-self.b)** 2 / 2. + self.f_reg(x)\n",
    "        elif self.loss=='log':\n",
    "            v = (self.b==0)\n",
    "            Ax = self.A.dot(x)\n",
    "            return v.dot(Ax)+ np.sum(np.log(1. + np.exp(-Ax))) + self.f_reg(x)\n",
    "            \n",
    "        \n",
    "    # Gradient de l'objectif du probleme (maximum de vraisemblance ou a posteriori)\n",
    "    def grad(self, x):\n",
    "\n",
    "        # Calcul du gradient de la fonction objectif en x (perte+regularisation)\n",
    "        if self.loss=='lin':\n",
    "            return self.A.T.dot(self.A.dot(x)-self.b) + self.g_reg(x)\n",
    "        elif self.loss=='log':\n",
    "            v = (self.b==0)\n",
    "            Av = self.A.T.dot(v)\n",
    "            Ax = self.A.dot(x)\n",
    "            return Av - self.A.T.dot(1./(1. + np.exp(Ax))) + self.g_reg(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On instancie ensuite la classe pour représenter les problèmes souhaités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation du modele\n",
    "d = 1\n",
    "n = 40\n",
    "idx = np.arange(d)\n",
    "mu = 1. / d ** (0.5) \n",
    "# Une valeur plus forte peut deteriorer la qualite de la regression, cf ci-dessous.\n",
    "#mu = 100\n",
    "\n",
    "# Verite terrain\n",
    "xstar = (-1)**idx * np.exp(-idx / 10.)\n",
    "\n",
    "\n",
    "# Generation des donnees pour la regression lineaire\n",
    "Alin = multivariate_normal(np.zeros(d), toeplitz(0.1 ** np.arange(0, d)), size=n)\n",
    "blin = Alin.dot(xstar) + normal(0,1,size=n) \n",
    "blinstar = Alin.dot(xstar)\n",
    "\n",
    "# Generation des donnees pour la regression logistique\n",
    "Alog = multivariate_normal(np.zeros(d), toeplitz(0.7 ** np.arange(0, d)), size=n)\n",
    "blog = Alog.dot(xstar) + normal(0,1,size=n) \n",
    "blog = np.maximum(np.sign(blog),0)\n",
    "blogstar = np.maximum(np.sign(Alog.dot(xstar)),0)\n",
    "\n",
    "# - Creation des instances\n",
    "lin_none = RegNonLin(Alin,blin,mu,loss='lin',reg='none')# Regression lineaire sans regularisation\n",
    "lin_gauss = RegNonLin(Alin,blin,mu,loss='lin',reg='gauss')# Regression lineaire avec regularisation\n",
    "log_none = RegNonLin(Alog,blog,mu,loss='log',reg='none')# Regression logistique sans regularisation\n",
    "log_gauss = RegNonLin(Alog,blog,mu,loss='log',reg='gauss')# Regression logistique avec regularisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le script ci-dessous teste finalement les problèmes de régression linéaire et les problèmes de régression logistique avec ou sans régularisation gaussienne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise ici L-BFGS-B pour determiner une solution au probleme initial\n",
    "x_init = np.zeros(d)\n",
    "\n",
    "# Regression lineaire sans regularisation\n",
    "x_lin_none, f_lin_none, _ = fmin_l_bfgs_b(lin_none.fun, x_init, lin_none.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "print('Reg. Lin. sans a priori:')\n",
    "print(' fmin= ', f_lin_none)\n",
    "print(' ngrad= ',norm(lin_none.grad(x_lin_none)))\n",
    "print(' ecart avec xstar : ',norm(xstar-x_lin_none))\n",
    "\n",
    "# Regression lineaire avec regularisation\n",
    "x_lin_gauss, f_lin_gauss, _ = fmin_l_bfgs_b(lin_gauss.fun, x_init, lin_gauss.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "print('Reg. Lin. avec a priori gaussien:')\n",
    "print(' fmin= ', f_lin_gauss)\n",
    "print(' ngrad= ',norm(lin_gauss.grad(x_lin_gauss)))\n",
    "print(' ecart avec xstar : ',norm(xstar-x_lin_gauss))\n",
    "\n",
    "# Regression logistique sans regularisation\n",
    "x_log_none, f_log_none, _ = fmin_l_bfgs_b(log_none.fun, x_init, log_none.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "b_log_none = np.maximum(np.sign(Alog.dot(x_log_none)),0)\n",
    "print('Reg. Log. sans a priori:')\n",
    "print(' fmin= ', f_log_none)\n",
    "print(' ngrad= ',norm(log_none.grad(x_log_none)))\n",
    "print(' ecart avec ylogstar : ',norm(blogstar-b_log_none,1))\n",
    "\n",
    "# Regression logistique avec regularisation\n",
    "x_log_gauss, f_log_gauss, _ = fmin_l_bfgs_b(log_gauss.fun, x_init, log_gauss.grad, args=(), pgtol=1e-30, factr =1e-30)\n",
    "b_log_gauss = np.maximum(np.sign(Alog.dot(x_log_gauss)),0)\n",
    "print('Reg. Log. avec a priori gaussien:')\n",
    "print(' fmin= ', f_log_gauss)\n",
    "print(' ngrad= ',norm(log_gauss.grad(x_log_gauss)))\n",
    "print(' ecart avec ylogstar : ',norm(blogstar-b_log_gauss,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if d==1:\n",
    "    # Affichage des donnees\n",
    "    plt.figure()\n",
    "    plt.scatter(Alin[:,0],blinstar,color='k',marker='o',label='Vrai modele')\n",
    "    plt.scatter(Alin[:,0],blin,color='red',marker='^',label='Donnees Lin')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage des predictions\n",
    "    plt.figure()\n",
    "    plt.plot(Alin[:,0],blinstar,color='k',label='Verite terrain')\n",
    "    plt.plot(Alin[:,0],Alin.dot(x_lin_none),color='red',label='MV Gauss')\n",
    "    plt.plot(Alin[:,0],Alin.dot(x_lin_gauss),color='crimson',label='MAP Gauss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage des donnees\n",
    "    plt.figure()\n",
    "    plt.scatter(Alog[:,0],blogstar,color='k',marker='o',label='Vrai modele')\n",
    "    plt.scatter(Alog[:,0],blog,color='steelblue',marker='d',label='Donnees Log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Affichage des predictions\n",
    "    plt.figure()\n",
    "    plt.scatter(Alog[:,0],blogstar,marker='o',color='k',label='Verite terrain')\n",
    "    plt.scatter(Alog[:,0],b_log_none,marker='d',color='steelblue',label='MV Log')\n",
    "    plt.scatter(Alog[:,0],b_log_gauss,marker='v',color='indigo',label='MAP Gauss')\n",
    "    plt.legend()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basé sur - C. W. Royer, octobre 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
